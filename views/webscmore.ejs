<html>
<head>
<style>
.div1122 {
  background-color: orange;
  border: 30px solid yellow;
  padding: 50px;
  margin: 20px;
}
body {
  background-image: url(bac.jpg);
}

</style>
</head>
<body>
<h1 align="center">Web Scraping</h1>
<div align="center" class="div1122">
<h3>What is web scraping</h3>
Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. The web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.

Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and telephone numbers, or companies and their URLs, or e-mail addresses to a list (contact scraping).

Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup, and web data integration.

Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. As a result, specialized tools and software have been developed to facilitate the scraping of web pages.

Newer forms of web scraping involve monitoring data feeds from web servers. For example, JSON is commonly used as a transport storage mechanism between the client and the web server.

There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing.
</div>
<br/><br/>
<div align="center" class="div1122">
<h3> History of Web Scraping</h3>
The history of the web scraping dates back nearly to the time when the World Wide Web was born.

After the birth of World Wide Web in 1989, the first web robot,[1] World Wide Web Wanderer, was created in June 1993, which was intended only to measure the size of the web.
In December 1993, the first crawler-based web search engine, JumpStation, was launched. As there were not so many websites available on the web, search engines at that time used to rely on their human website administrators to collect and edit the links into a particular format. In comparison, JumpStation brought a new leap, being the first WWW search engine that relied on a web robot.
In 2000, the first Web API and API crawler came. API stands for Application Programming Interface. It is an interface that makes it much easier to develop a program by providing the building blocks. In 2000, Salesforce and eBay launched their own API, with which programmers were enabled to access and download some of the data available to the public. Since then, many websites offer web APIs for people to access their public database.
</div>
<br/><br/>
<div align="center" class="div1122">
<h3>Applications of Web Scraping</h3>
<b>Social media sentiment analysis</b>
The shelf life of social media posts is very little, however, when looked at collectively they show valuable trends. While most social media platforms have APIs that let 3rd party tools access their data, this may not always be sufficient. In such cases scraping these websites gives access to real-time information such as trending sentiments, phrases, topics, etc. 
<br/><br/>
<b>eCommerce pricing</b>
Many eCommerce sellers often have their products listed on multiple marketplaces. With scraping, they can monitor the pricing on multiple platforms and make a sale on the marketplace where the profit is higher. 
<br/><br/>
<b>Investment opportunities</b>
Real estate investors often want to know about promising neighborhoods they can invest in. While there are multiple ways to get this data, web scraping travel marketplaces and hospitality brokerage websites offer valuable information. This includes information such as the highest-rated areas, amenities that typical buyers look for, locations that may be upcoming as attractive renting options, etc.  
<br/><br/>
<b>Machine learning</b>
Machine learning models need raw data to evolve and improve. Web scraping tools can scrape a large number of data points, text and images in a relatively short time. Machine learning is fueling todayâ€™s technological marvels such as driverless cars, space flight, image and speech recognition. However, these models need data to improve their accuracy and reliability. 
A good web scraping project follows these practices. These ensure that you get the data you are looking for while being non-disruptive to the data sources. 
<br/><br/>
<b>Identify the goal</b>
Any web scraping project begins with a need. A goal detailing the expected outcomes is necessary and is the most basic need for a scraping task. The following set of questions need to be asked while identifying the need for a web scraping project:

What kind of information do we expect to seek?
What will be the outcome of this scraping activity?
Where is this information typically published?
Who are the end-users who will consume this data?
Where will the extracted data be stored? For e.g. on Cloud or on-premise storage, on an external database, etc.
How should this data be presented to its end-users? For e.g. as a CSV/Excel/JSON file or as an SQL database, etc.
How often are the source websites refreshed with new data? In other words, what is the typical shelf-life of the data that is being collected and how often does it have to be refreshed?
Post the scraping activity, what are the types of reports you would want to generate?
</body>
</html>